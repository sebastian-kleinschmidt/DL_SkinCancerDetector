{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist AI - Skin Cancer Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#Check if GPU is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# how many samples per batch to load\n",
    "num_classes = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Declare augmentation transformations which will be applied to all input images of the trainset\n",
    "transform_augm = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(30),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.75, 1.0), ratio=(0.85, 1.15)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Declare transformations which will be applied to all input images of the test and validationset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((230,230)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "#Declare datasets\n",
    "data_segmentation = {}\n",
    "data_segmentation['train'] = datasets.ImageFolder('data/segmentation/train/',transform=transform)\n",
    "data_segmentation['valid'] = datasets.ImageFolder('data/segmentation/valid/',transform=transform)\n",
    "#data_segmentation['test'] = datasets.ImageFolder('data/segmentation/test/',transform=transform)\n",
    "\n",
    "data_classfication = {}\n",
    "data_classfication['train'] = datasets.ImageFolder('data/classification/train/',transform=transform_augm)\n",
    "data_classfication['valid'] = datasets.ImageFolder('data/classification/valid/',transform=transform)\n",
    "data_classfication['test'] = datasets.ImageFolder('data/classification/test/',transform=transform)\n",
    "\n",
    "#Declare loader\n",
    "loaders_segmentation = {}\n",
    "loaders_segmentation['train'] = torch.utils.data.DataLoader(data_segmentation['train'], shuffle=True, batch_size=batch_size)\n",
    "loaders_segmentation['valid'] = torch.utils.data.DataLoader(data_segmentation['valid'], shuffle=True, batch_size=batch_size)\n",
    "#loaders_segmentation['test'] = torch.utils.data.DataLoader(data_segmentation['test'], shuffle=False, batch_size=batch_size)\n",
    "\n",
    "loaders_classfication = {}\n",
    "loaders_classfication['train'] = torch.utils.data.DataLoader(data_classfication['train'], shuffle=True, batch_size=batch_size)\n",
    "loaders_classfication['valid'] = torch.utils.data.DataLoader(data_classfication['valid'], shuffle=True, batch_size=batch_size)\n",
    "loaders_classfication['test'] = torch.utils.data.DataLoader(data_classfication['test'], shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #Activation function\n",
    "        self.leakyRelu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(3, 16, 5, padding=2, stride=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.pool1_1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(16, 32, 5, padding=2,  stride=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.pool2_1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(64)\n",
    "        self.pool3_1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(128)\n",
    "        self.pool4_1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv5_bn = nn.BatchNorm2d(256)\n",
    "        self.pool5_1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*256, 200)\n",
    "        self.fc1_bn = nn.BatchNorm1d(200)\n",
    "        self.dropoutfc = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(200, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = self.leakyRelu(self.conv1_1(x))\n",
    "        x = self.conv1_bn(x)\n",
    "        x = self.pool1_1(x)\n",
    "        \n",
    "        x = self.leakyRelu(self.conv2_1(x))\n",
    "        x = self.conv2_bn(x)\n",
    "        x = self.pool2_1(x)\n",
    "        \n",
    "        x = self.leakyRelu(self.conv3_1(x))\n",
    "        x = self.conv3_bn(x)\n",
    "        x = self.pool3_1(x)\n",
    "        \n",
    "        x = self.leakyRelu(self.conv4_1(x))\n",
    "        x = self.pool4_1(x)\n",
    "        x = self.conv4_bn(x)\n",
    "        \n",
    "        x = self.leakyRelu(self.conv5_1(x))\n",
    "        x = self.pool5_1(x)\n",
    "        x = self.conv5_bn(x)\n",
    "        \n",
    "        x = x.view(-1,7*7*256)\n",
    "        x = self.leakyRelu(self.fc1(x))\n",
    "        x = self.dropoutfc(x)\n",
    "        x = self.leakyRelu(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.Adam(model_scratch.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate the Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-31d8c6351c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mmodel_scratch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_classfication\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_scratch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;31m# load the model that got the best validation accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-31d8c6351c93>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"\"\"returns trained model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# initialize tracker for minimum validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvalid_loss_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "\n",
    "            # move to GPU            \n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            # update the average validation loss\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "#Continiue Training if file already exists\n",
    "if os.path.isfile('model.pt') :\n",
    "    model_scratch.load_state_dict(torch.load('model.pt'))\n",
    "# train the model\n",
    "model = train(10, loaders_classfication, model_scratch, optimizer, criterion, use_cuda, 'model.pt')\n",
    "# load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
